
@article{hines_fully_2008,
	title = {Fully implicit parallel simulation of single neurons},
	volume = {25},
	issn = {0929-5313, 1573-6873},
	url = {http://link.springer.com/article/10.1007/s10827-008-0087-5},
	doi = {10.1007/s10827-008-0087-5},
	abstract = {When a multi-compartment neuron is divided into subtrees such that no subtree has more than two connection points to other subtrees, the subtrees can be on different processors and the entire system remains amenable to direct Gaussian elimination with only a modest increase in complexity. Accuracy is the same as with standard Gaussian elimination on a single processor. It is often feasible to divide a 3-D reconstructed neuron model onto a dozen or so processors and experience almost linear speedup. We have also used the method for purposes of load balance in network simulations when some cells are so large that their individual computation time is much longer than the average processor computation time or when there are many more processors than cells. The method is available in the standard distribution of the NEURON simulation program.},
	language = {en},
	number = {3},
	urldate = {2015-01-13},
	journal = {Journal of Computational Neuroscience},
	author = {Hines, Michael L. and Markram, Henry and Schürmann, Felix},
	month = dec,
	year = {2008},
	keywords = {Computer modeling, Computer simulation, Human Genetics, Load balance, Neurology, Neuronal networks, Neurosciences, Parallel simulation, Theory of Computation},
	pages = {439--448},
	file = {Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/E74IDMM7/Hines et al. - 2008 - Fully implicit parallel simulation of single neuro.pdf:application/pdf;Snapshot:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/KQIBXV5K/s10827-008-0087-5.html:text/html}
}

@article{migliore_parallel_2006,
	title = {Parallel network simulations with {NEURON}},
	volume = {21},
	issn = {0929-5313, 1573-6873},
	url = {http://link.springer.com/article/10.1007/s10827-006-7949-5},
	doi = {10.1007/s10827-006-7949-5},
	abstract = {The NEURON simulation environment has been extended to support parallel network simulations. Each processor integrates the equations for its subnet over an interval equal to the minimum (interprocessor) presynaptic spike generation to postsynaptic spike delivery connection delay. The performance of three published network models with very different spike patterns exhibits superlinear speedup on Beowulf clusters and demonstrates that spike communication overhead is often less than the benefit of an increased fraction of the entire problem fitting into high speed cache. On the EPFL IBM Blue Gene, almost linear speedup was obtained up to 100 processors. Increasing one model from 500 to 40,000 realistic cells exhibited almost linear speedup on 2000 processors, with an integration time of 9.8 seconds and communication time of 1.3 seconds. The potential for speed-ups of several orders of magnitude makes practical the running of large network simulations that could otherwise not be explored.},
	language = {en},
	number = {2},
	urldate = {2014-11-21},
	journal = {Journal of Computational Neuroscience},
	author = {Migliore, M. and Cannia, C. and Lytton, W. W. and Markram, Henry and Hines, M. L.},
	month = oct,
	year = {2006},
	keywords = {Computer simulation, Human Genetics, Neurology, Neurosciences, Parallel computation, Realistic modeling, Spiking networks, Theory of Computation},
	pages = {119--129},
	file = {Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/SNKB7C9A/Migliore et al. - 2006 - Parallel network simulations with NEURON.pdf:application/pdf;Snapshot:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/IRK47QJF/s10827-006-7949-5.html:text/html}
}

@article{hines_translating_2008,
	series = {Methods for {Computational} {Neuroscience}},
	title = {Translating network models to parallel hardware in {NEURON}},
	volume = {169},
	issn = {0165-0270},
	url = {http://www.sciencedirect.com/science/article/pii/S0165027007004608},
	doi = {10.1016/j.jneumeth.2007.09.010},
	abstract = {The increasing complexity of network models poses a growing computational burden. At the same time, computational neuroscientists are finding it easier to access parallel hardware, such as multiprocessor personal computers, workstation clusters, and massively parallel supercomputers. The practical question is how to move a working network model from a single processor to parallel hardware. Here we show how to make this transition for models implemented with NEURON, in such a way that the final result will run and produce numerically identical results on either serial or parallel hardware. This allows users to develop and debug models on readily available local resources, then run their code without modification on a parallel supercomputer.},
	number = {2},
	urldate = {2015-01-17},
	journal = {Journal of Neuroscience Methods},
	author = {Hines, M. L. and Carnevale, N. T.},
	month = apr,
	year = {2008},
	keywords = {Computational neuroscience, Multiprocessor, Network model, NEURON simulation environment, Parallel computation, Parallel supercomputer, Serial computation, Simulation},
	pages = {425--455},
	file = {ScienceDirect Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/265729VB/Hines and Carnevale - 2008 - Translating network models to parallel hardware in.pdf:application/pdf;ScienceDirect Snapshot:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/EBFS6G79/S0165027007004608.html:text/html}
}

@inproceedings{kumar_optimization_2010,
	title = {Optimization of applications with non-blocking neighborhood collectives via multisends on the {Blue} {Gene}/{P} supercomputer},
	doi = {10.1109/IPDPS.2010.5470407},
	abstract = {We explore the multisend interface as a data mover interface to optimize applications with neighborhood collective communication operations. One of the limitations of the current MPI 2.1 standard is that the vector collective calls require counts and displacements (zero and non-zero bytes) to be specified for all the processors in the communicator. Further, all the collective calls in MPI 2.1 are blocking and do not permit overlap of communication with computation in the same thread of execution. However, multisends are non-blocking calls that permit overlap of computation and communication. We present the record replay persistent optimization to the multisend interface th at minimizes the processor overhead of initiating the collective. We present four different case studies with the multisend API on Blue Gene/P (i) 3D-FFT, (ii) 4D nearest neighbor exchange as used in Quantum Chromodynamics, (iii) NAMD and (iv) neural network simulator NEURON. Performance results show 1.9Ã speedup with 323 3D-FFTs, 1.9Ã speedup for 4D nearest neighbor exchange with the 24 problem, 1.6Ã speedup in NAMD and almost 3Ã speedup in NEURON with 256K cells and 1k connections/cell.},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Parallel} {Distributed} {Processing} ({IPDPS})},
	author = {Kumar, S. and Heidelberger, P. and Chen, Dong and Hines, M.},
	month = apr,
	year = {2010},
	keywords = {3D-FFT, 4D nearest neighbor exchange, application optimization, application program interfaces, Application software, Blue Gene/P supercomputer, Buffer storage, Communication standards, Computer science, data mover interface, fast Fourier transforms, LIBRARIES, Message passing, MPI 2.1 standard, multiprocessing systems, multisend API, multisend interface, NAMD, Nearest neighbor searches, neighborhood collective communication operation, neural nets, Neural networks, neural network simulator, NEURON, Neurons, nonblocking neighborhood collectives, processor overhead minimization, Proposals, quantum chromodynamics, record replay persistent optimization, Supercomputers, vector collective calls},
	pages = {1--11},
	file = {IEEE Xplore Abstract Record:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/K8SNZR58/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/GAUCRRDX/Kumar et al. - 2010 - Optimization of applications with non-blocking nei.pdf:application/pdf}
}

@article{migliore_distributed_2014,
	title = {Distributed organization of a brain microcircuit analyzed by three-dimensional modeling: the olfactory bulb},
	volume = {8},
	issn = {1662-5188},
	shorttitle = {Distributed organization of a brain microcircuit analyzed by three-dimensional modeling},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4010739/},
	doi = {10.3389/fncom.2014.00050},
	abstract = {The functional consequences of the laminar organization observed in cortical systems cannot be easily studied using standard experimental techniques, abstract theoretical representations, or dimensionally reduced models built from scratch. To solve this problem we have developed a full implementation of an olfactory bulb microcircuit using realistic three-dimensional (3D) inputs, cell morphologies, and network connectivity. The results provide new insights into the relations between the functional properties of individual cells and the networks in which they are embedded. To our knowledge, this is the first model of the mitral-granule cell network to include a realistic representation of the experimentally-recorded complex spatial patterns elicited in the glomerular layer (GL) by natural odor stimulation. Although the olfactory bulb, due to its organization, has unique advantages with respect to other brain systems, the method is completely general, and can be integrated with more general approaches to other systems. The model makes experimentally testable predictions on distributed processing and on the differential backpropagation of somatic action potentials in each lateral dendrite following odor learning, providing a powerful 3D framework for investigating the functions of brain microcircuits.},
	urldate = {2015-03-02},
	journal = {Frontiers in Computational Neuroscience},
	author = {Migliore, Michele and Cavarretta, Francesco and Hines, Michael L. and Shepherd, Gordon M.},
	month = apr,
	year = {2014},
	pmid = {24808855},
	pmcid = {PMC4010739},
	file = {PubMed Central Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/HKJVZKAW/Migliore et al. - 2014 - Distributed organization of a brain microcircuit a.pdf:application/pdf}
}

@article{hines_comparison_2011,
	title = {Comparison of neuronal spike exchange methods on a {Blue} {Gene}/{P} supercomputer},
	volume = {5},
	issn = {1662-5188},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3219917/},
	doi = {10.3389/fncom.2011.00049},
	abstract = {For neural network simulations on parallel machines, interprocessor spike communication can be a significant portion of the total simulation time. The performance of several spike exchange methods using a Blue Gene/P (BG/P) supercomputer has been tested with 8–128 K cores using randomly connected networks of up to 32 M cells with 1 k connections per cell and 4 M cells with 10 k connections per cell, i.e., on the order of 4·1010 connections (K is 1024, M is 10242, and k is 1000). The spike exchange methods used are the standard Message Passing Interface (MPI) collective, MPI\_Allgather, and several variants of the non-blocking Multisend method either implemented via non-blocking MPI\_Isend, or exploiting the possibility of very low overhead direct memory access (DMA) communication available on the BG/P. In all cases, the worst performing method was that using MPI\_Isend due to the high overhead of initiating a spike communication. The two best performing methods—the persistent Multisend method using the Record-Replay feature of the Deep Computing Messaging Framework DCMF\_Multicast; and a two-phase multisend in which a DCMF\_Multicast is used to first send to a subset of phase one destination cores, which then pass it on to their subset of phase two destination cores—had similar performance with very low overhead for the initiation of spike communication. Departure from ideal scaling for the Multisend methods is almost completely due to load imbalance caused by the large variation in number of cells that fire on each processor in the interval between synchronization. Spike exchange time itself is negligible since transmission overlaps with computation and is handled by a DMA controller. We conclude that ideal performance scaling will be ultimately limited by imbalance between incoming processor spikes between synchronization intervals. Thus, counterintuitively, maximization of load balance requires that the distribution of cells on processors should not reflect neural net architecture but be randomly distributed so that sets of cells which are burst firing together should be on different processors with their targets on as large a set of processors as possible.},
	urldate = {2015-03-02},
	journal = {Frontiers in Computational Neuroscience},
	author = {Hines, Michael and Kumar, Sameer and Schürmann, Felix},
	month = nov,
	year = {2011},
	pmid = {22121345},
	pmcid = {PMC3219917},
	file = {PubMed Central Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/JICA7GGQ/Hines et al. - 2011 - Comparison of neuronal spike exchange methods on a.pdf:application/pdf}
}

@article{brette_simulation_2007,
	title = {Simulation of networks of spiking neurons: {A} review of tools and strategies},
	volume = {23},
	issn = {1573-6873},
	shorttitle = {Simulation of networks of spiking neurons},
	url = {http://link.springer.com/article/10.1007/s10827-007-0038-6},
	doi = {10.1007/s10827-007-0038-6},
	abstract = {We review different aspects of the simulation of spiking neural networks. We start by reviewing the different types of simulation strategies and algorithms that are currently implemented. We next review the precision of those simulation strategies, in particular in cases where plasticity depends on the exact timing of the spikes. We overview different simulators and simulation environments presently available (restricted to those freely available, open source and documented). For each simulation tool, its advantages and pitfalls are reviewed, with an aim to allow the reader to identify which simulator is appropriate for a given task. Finally, we provide a series of benchmark simulations of different types of networks of spiking neurons, including Hodgkin–Huxley type, integrate-and-fire models, interacting with current-based or conductance-based synapses, using clock-driven or event-driven integration strategies. The same set of models are implemented on the different simulators, and the codes are made available. The ultimate goal of this review is to provide a resource to facilitate identifying the appropriate integration strategy and simulation tool to use for a given modeling problem related to spiking neural networks.},
	language = {en},
	number = {3},
	urldate = {2015-03-02},
	journal = {Journal of Computational Neuroscience},
	author = {Brette, Romain and Rudolph, Michelle and Carnevale, Ted and Hines, Michael and Beeman, David and Bower, James M. and Diesmann, Markus and Morrison, Abigail and Goodman, Philip H. and Jr, Frederick C. Harris and Zirpe, Milind and Natschläger, Thomas and Pecevski, Dejan and Ermentrout, Bard and Djurfeldt, Mikael and Lansner, Anders and Rochel, Olivier and Vieville, Thierry and Muller, Eilif and Davison, Andrew P. and Boustani, Sami El and Destexhe, Alain},
	month = jul,
	year = {2007},
	keywords = {Clock-driven, Event-driven, Human Genetics, Integration strategies, Neurology, Neurosciences, Simulation tools, Spiking neural networks, Theory of Computation},
	pages = {349--398},
	file = {Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/UT8D2QX8/Brette et al. - 2007 - Simulation of networks of spiking neurons A revie.pdf:application/pdf;Snapshot:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/KZ2MTH8I/s10827-007-0038-6.html:text/html}
}

@article{hines_neuron_2008,
	title = {Neuron splitting in compute-bound parallel network simulations enables runtime scaling with twice as many processors},
	volume = {25},
	issn = {0929-5313, 1573-6873},
	url = {http://link.springer.com/article/10.1007/s10827-007-0073-3},
	doi = {10.1007/s10827-007-0073-3},
	abstract = {Neuron tree topology equations can be split into two subtrees and solved on different processors with no change in accuracy, stability, or computational effort; communication costs involve only sending and receiving two double precision values by each subtree at each time step. Splitting cells is useful in attaining load balance in neural network simulations, especially when there is a wide range of cell sizes and the number of cells is about the same as the number of processors. For compute-bound simulations load balance results in almost ideal runtime scaling. Application of the cell splitting method to two published network models exhibits good runtime scaling on twice as many processors as could be effectively used with whole-cell balancing.},
	language = {en},
	number = {1},
	urldate = {2015-03-02},
	journal = {Journal of Computational Neuroscience},
	author = {Hines, Michael L. and Eichner, Hubert and Schürmann, Felix},
	month = jan,
	year = {2008},
	keywords = {Computer modeling, Computer simulation, Human Genetics, Load balance, Neurology, Neuronal networks, Neurosciences, Parallel simulation, Theory of Computation},
	pages = {203--210},
	file = {Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/Z7A7WNUN/Hines et al. - 2008 - Neuron splitting in compute-bound parallel network.pdf:application/pdf;Snapshot:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/3V7CB5II/s10827-007-0073-3.html:text/html}
}

@article{hines_modeldb:_2004,
	title = {{ModelDB}: {A} {Database} to {Support} {Computational} {Neuroscience}},
	volume = {17},
	issn = {0929-5313, 1573-6873},
	shorttitle = {{ModelDB}},
	url = {http://link.springer.com/article/10.1023/B%3AJCNS.0000023869.22017.2e},
	doi = {10.1023/B:JCNS.0000023869.22017.2e},
	abstract = {Wider dissemination and testing of computational models are crucial to the field of computational neuroscience. Databases are being developed to meet this need. ModelDB is a web-accessible database for convenient entry, retrieval, and running of published models on different platforms. This article provides a guide to entering a new model into ModelDB.},
	language = {en},
	number = {1},
	urldate = {2015-07-29},
	journal = {Journal of Computational Neuroscience},
	author = {Hines, Michael L. and Morse, Thomas and Migliore, Michele and Carnevale, Nicholas T. and Shepherd, Gordon M.},
	month = jul,
	year = {2004},
	keywords = {database model, Human Genetics, Neurology, Neurosciences, Theory of Computation},
	pages = {7--11},
	file = {Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/6TC3V8T4/Hines et al. - 2004 - ModelDB A Database to Support Computational Neuro.pdf:application/pdf;Snapshot:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/97T7PAUK/BJCNS.0000023869.22017.html:text/html}
}

@article{hines_neuron_1997,
	title = {The {NEURON} {Simulation} {Environment}},
	volume = {9},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/neco.1997.9.6.1179},
	doi = {10.1162/neco.1997.9.6.1179},
	abstract = {The moment-to-moment processing of information by the nervous system involves the propagation and interaction of electrical and chemical signals that are distributed in space and time. Biologically realistic modeling is needed to test hypotheses about the mechanisms that govern these signals and how nervous system function emerges from the operation of these mechanisms. The NEURON simulation program provides a powerful and flexible environment for implementing such models of individual neurons and small networks of neurons. It is particularly useful when membrane potential is nonuniform and membrane currents are complex. We present the basic ideas that would help informed users make the most efficient use of NEURON.},
	number = {6},
	urldate = {2015-08-05},
	journal = {Neural Computation},
	author = {Hines, M. L. and Carnevale, N. T.},
	month = aug,
	year = {1997},
	pages = {1179--1209},
	file = {Neural Computation Snapshot:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/5EG8P2IB/neco.1997.9.6.html:text/html}
}

@inproceedings{broquedis_hwloc:_2010,
	title = {hwloc: {A} {Generic} {Framework} for {Managing} {Hardware} {Affinities} in {HPC} {Applications}},
	shorttitle = {hwloc},
	doi = {10.1109/PDP.2010.67},
	abstract = {The increasing numbers of cores, shared caches and memory nodes within machines introduces a complex hardware topology. High-performance computing applications now have to carefully adapt their placement and behavior according to the underlying hierarchy of hardware resources and their software affinities. We introduce the Hardware Locality (hwloc) software which gathers hardware information about processors, caches, memory nodes and more, and exposes it to applications and runtime systems in a abstracted and portable hierarchical manner. hwloc may significantly help performance by having runtime systems place their tasks or adapt their communication strategies depending on hardware affinities. We show that hwloc can already be used by popular high-performance OpenMP or MPI software. Indeed, scheduling OpenMP threads according to their affinities or placing MPI processes according to their communication patterns shows interesting performance improvement thanks to hwloc. An optimized MPI communication strategy may also be dynamically chosen according to the location of the communicating processes in the machine and its hardware characteristics.},
	booktitle = {2010 18th {Euromicro} {International} {Conference} on {Parallel}, {Distributed} and {Network}-{Based} {Processing} ({PDP})},
	author = {Broquedis, F. and Clet-Ortega, J. and Moreaud, S. and Furmento, N. and Goglin, B. and Mercier, G. and Thibault, S. and Namyst, R.},
	month = feb,
	year = {2010},
	keywords = {application program interfaces, Application software, Bandwidth, complex hardware topology, Computer architecture, Concurrent computing, Conference management, Hardware, hardware affinities management, hardware locality software, Hardware Topology Affinities Placement MPI OpenMP, high-performance computing, hwloc, Memory management, memory nodes, Message passing, MPI software, multicore processor, multi-threading, OpenMP thread scheduling, runtime system, scheduling, shared caches, software affinity, Software libraries, Topology, Yarn},
	pages = {180--186},
	file = {IEEE Xplore Abstract Record:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/NKXJCANK/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/455WC9MR/Broquedis et al. - 2010 - hwloc A Generic Framework for Managing Hardware A.pdf:application/pdf}
}

@article{davison_dendrodendritic_2003,
	title = {Dendrodendritic {Inhibition} and {Simulated} {Odor} {Responses} in a {Detailed} {Olfactory} {Bulb} {Network} {Model}},
	volume = {90},
	copyright = {Copyright © 2003 by the American Physiological Society},
	issn = {0022-3077, 1522-1598},
	url = {http://jn.physiology.org/content/90/3/1921},
	doi = {10.1152/jn.00623.2002},
	abstract = {In the olfactory bulb, both the spatial distribution and the temporal structure of neuronal activity appear to be important for processing odor information, but it is currently impossible to measure both of these simultaneously with high resolution and in all layers of the bulb. We have developed a biologically realistic model of the mammalian olfactory bulb, incorporating the mitral and granule cells and the dendrodendritic synapses between them, which allows us to observe the network behavior in detail. The cell models were based on previously published work. The attributes of the synapses were obtained from the literature. The pattern of synaptic connections was based on the limited experimental data in the literature on the statistics of connections between neurons in the bulb. The results of simulation experiments with electrical stimulation agree closely in most details with published experimental data. This gives confidence that the model is capturing features of network interactions in the real olfactory bulb. The model predicts that the time course of dendrodendritic inhibition is dependent on the network connectivity as well as on the intrinsic parameters of the synapses. In response to simulated odor stimulation, strongly activated mitral cells tend to suppress neighboring cells, the mitral cells readily synchronize their firing, and increasing the stimulus intensity increases the degree of synchronization. Preliminary experiments suggest that slow temporal changes in the degree of synchronization are more useful in distinguishing between very similar odorants than is the spatial distribution of mean firing rate.},
	language = {en},
	number = {3},
	urldate = {2015-06-24},
	journal = {Journal of Neurophysiology},
	author = {Davison, Andrew P. and Feng, Jianfeng and Brown, David},
	month = sep,
	year = {2003},
	pmid = {12736241},
	pages = {1921--1935},
	file = {Full Text PDF:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/WPFPBCFM/Davison et al. - 2003 - Dendrodendritic Inhibition and Simulated Odor Resp.pdf:application/pdf;Snapshot:/home/gokce/.mozilla/firefox/ggjhslj6.default-1408233091542/zotero/storage/3MBEMPH7/1921.html:text/html}
}

@article{karypis_fast_1998,
	title = {A {Fast} and {High} {Quality} {Multilevel} {Scheme} for {Partitioning} {Irregular} {Graphs}},
	volume = {20},
	issn = {1064-8275},
	url = {http://dx.doi.org/10.1137/S1064827595287997},
	doi = {10.1137/S1064827595287997},
	abstract = {Recently, a number of researchers have investigated a class of graph partitioning algorithms that reduce the size of the graph by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph [Bui and Jones, Proc. of the 6th SIAM Conference on Parallel Processing for Scientific Computing, 1993, 445--452; Hendrickson and Leland,  A Multilevel Algorithm for Partitioning Graphs, Tech. report SAND 93-1301, Sandia National Laboratories, Albuquerque, NM, 1993]. From the early work it was clear that multilevel techniques held great promise; however, it was not known if they can be made to consistently produce high quality partitions for graphs arising in a wide range of application domains. We investigate the effectiveness of many different choices for all three phases: coarsening, partition of the coarsest graph, and refinement. In particular, we present a new coarsening heuristic (called heavy-edge heuristic) for which the size of the partition of the coarse graph is within a small factor of the size of the final partition obtained after multilevel refinement. We also present a much faster variation of the Kernighan--Lin (KL) algorithm for refining during uncoarsening. We test our scheme on a large number of graphs arising in various domains including finite element methods, linear programming, VLSI, and transportation. Our experiments show that our scheme produces partitions that are consistently better than those produced by spectral partitioning schemes in substantially smaller time. Also, when our scheme is used to compute fill-reducing orderings for sparse matrices, it produces orderings that have substantially smaller fill than the widely used multiple minimum degree algorithm.},
	number = {1},
	urldate = {2015-06-24},
	journal = {SIAM J. Sci. Comput.},
	author = {Karypis, George and Kumar, Vipin},
	month = dec,
	year = {1998},
	keywords = {fill-reducing orderings, finite element computations, graph partitioning, parallel computations},
	pages = {359--392}
}